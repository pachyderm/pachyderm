syntax = "proto3";

package pps_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/pps";

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/duration.proto";

import "gogoproto/gogo.proto";

import "pfs/pfs.proto";

message SecretMount {
  // Name must be the name of the secret in kubernetes.
  string name = 1;
  // Key of the secret to load into env_var, this field only has meaning if EnvVar != "".
  string key = 2;
  string mount_path = 3;
  string env_var = 4;
}

message Transform {
  string image = 1;
  repeated string cmd = 2;
  repeated string err_cmd = 3;
  map<string, string> env = 4;
  repeated SecretMount secrets = 5;
  repeated string image_pull_secrets = 6;
  repeated string stdin = 7;
  repeated string err_stdin = 8;
  repeated int64 accept_return_code = 9;
  bool debug = 10;
  string user = 11;
  string working_dir = 12;
  string dockerfile = 13;
}

message TFJob {
  // tf_job  is a serialized Kubeflow TFJob spec. Pachyderm sends this directly
  // to a kubernetes cluster on which kubeflow has been installed, instead of
  // creating a pipeline ReplicationController as it normally would.
  string tf_job = 1 [(gogoproto.customname) = "TFJob"];
}

message Egress {
  string URL = 1;
}

message Job {
  Pipeline pipeline = 1;
  string id = 2 [(gogoproto.customname) = "ID"];
}

enum JobState {
  JOB_CREATED = 0;
  JOB_STARTING = 1;
  JOB_RUNNING = 2;
  JOB_FAILURE = 3;
  JOB_SUCCESS = 4;
  JOB_KILLED = 5;
  JOB_EGRESSING = 6;
}

message Metadata {
  map<string, string> annotations = 1;
  map<string, string> labels = 2;
}

message Service {
  int32 internal_port = 1;
  int32 external_port = 2;
  string ip = 3 [(gogoproto.customname) = "IP"];
  string type = 4;
}

message Spout {
  Service service = 1;
}

message PFSInput {
  string name = 1;
  string repo = 2;
  string repo_type = 13;
  string branch = 3;
  string commit = 4;
  string glob = 5;
  string join_on = 6;
  bool outer_join = 7;
  string group_by = 8;
  bool lazy = 9;
  // EmptyFiles, if true, will cause files from this PFS input to be
  // presented as empty files. This is useful in shuffle pipelines where you
  // want to read the names of files and reorganize them using symlinks.
  bool empty_files = 10;
  // S3, if true, will cause the worker to NOT download or link files from this
  // input into the /pfs_v2 directory. Instead, an instance of our S3 gateway
  // service will run on each of the sidecars, and data can be retrieved from
  // this input by querying
  // http://<pipeline>-s3.<namespace>/<job id>.<input>/my/file
  bool s3 = 11;
  // Trigger defines when this input is processed by the pipeline, if it's nil
  // the input is processed anytime something is committed to the input branch.
  pfs_v2.Trigger trigger = 12;
}

message CronInput {
  string name = 1;
  string repo = 2;
  string repo_type = 13;
  string commit = 3;
  string spec = 4;
  // Overwrite, if true, will expose a single datum that gets overwritten each
  // tick. If false, it will create a new datum for each tick.
  bool overwrite = 5;
  google.protobuf.Timestamp start = 6;
}


message Input {
  PFSInput pfs = 1;
  repeated Input join = 2;
  repeated Input group = 3;
  repeated Input cross = 4;
  repeated Input union = 5;
  CronInput cron = 6;
}

message JobInput {
  string name = 1;
  pfs_v2.Commit commit = 2;
  string glob = 3;
  bool lazy = 4;
}

message ParallelismSpec {
  // Starts the pipeline/job with a 'constant' workers, unless 'constant' is
  // zero. If 'constant' is zero (which is the zero value of ParallelismSpec),
  // then Pachyderm will choose the number of workers that is started,
  // (currently it chooses the number of workers in the cluster)
  uint64 constant = 1;
}

message InputFile {
  // This file's absolute path within its pfs repo.
  string path = 1;

  // This file's hash
  bytes hash = 2;
}

message Datum {
  // ID is the hash computed from all the files
  Job job = 1;
  string id = 2 [(gogoproto.customname) = "ID"];
}

enum DatumState {
  FAILED = 0;
  SUCCESS = 1;
  SKIPPED = 2;
  STARTING = 3;
  RECOVERED = 4;
  UNPROCESSED = 5;
}

message DatumInfo {
  Datum datum = 1;
  DatumState state = 2;
  ProcessStats stats = 3;
  pfs_v2.File pfs_state = 4;
  repeated pfs_v2.FileInfo data = 5;
}

message Aggregate {
  int64 count = 1;
  double mean = 2;
  double stddev = 3;
  double fifth_percentile = 4;
  double ninety_fifth_percentile = 5;
}

message ProcessStats {
  google.protobuf.Duration download_time = 1;
  google.protobuf.Duration process_time = 2;
  google.protobuf.Duration upload_time = 3;
  uint64 download_bytes = 4;
  uint64 upload_bytes = 5;
}

message AggregateProcessStats {
  Aggregate download_time = 1;
  Aggregate process_time = 2;
  Aggregate upload_time = 3;
  Aggregate download_bytes = 4;
  Aggregate upload_bytes = 5;
}

message WorkerStatus {
  string worker_id = 1 [(gogoproto.customname) = "WorkerID"];
  string job_id = 2 [(gogoproto.customname) = "JobID"];
  DatumStatus datum_status = 3;
}

message DatumStatus {
  // Started is the time processing on the current datum began.
  google.protobuf.Timestamp started = 1;
  repeated InputFile data = 2;
}

// ResourceSpec describes the amount of resources that pipeline pods should
// request from kubernetes, for scheduling.
message ResourceSpec {
  // The number of CPUs each worker needs (partial values are allowed, and
  // encouraged)
  float cpu = 1;

  // The amount of memory each worker needs (in bytes, with allowed
  // SI suffixes (M, K, G, Mi, Ki, Gi, etc).
  string memory = 2;

  // The spec for GPU resources.
  GPUSpec gpu = 3;

  // The amount of ephemeral storage each worker needs (in bytes, with allowed
  // SI suffixes (M, K, G, Mi, Ki, Gi, etc).
  string disk = 4;
}

message GPUSpec {
  // The type of GPU (nvidia.com/gpu or amd.com/gpu for example).
  string type = 1;
  // The number of GPUs to request.
  int64 number = 2;
}

// StoredJobInfo is the portion of the JobInfo that gets stored
// in the database during job execution. It contains fields which change over
// the lifetime of the job but aren't used in the execution of the job.
message StoredJobInfo {
  Job job = 1;
  uint64 pipeline_version = 2;
  pfs_v2.Commit output_commit = 3;
  // Job restart count (e.g. due to datum failure)
  uint64 restart = 4;

  // Counts of how many times we processed or skipped a datum
  int64 data_processed = 5;
  int64 data_skipped = 6;
  int64 data_total = 7;
  int64 data_failed = 8;
  int64 data_recovered = 9;

  // Download/process/upload time and download/upload bytes
  ProcessStats stats = 10;

  JobState state = 11;
  string reason = 12;
  google.protobuf.Timestamp started = 13;
  google.protobuf.Timestamp finished = 14;
}

message JobInfo {
  Job job = 1;
  Transform transform = 2;                     // requires ListJobRequest.Full
  uint64 pipeline_version = 3;
  ParallelismSpec parallelism_spec = 4;       // requires ListJobRequest.Full
  Egress egress = 5;                          // requires ListJobRequest.Full
  Job parent_job = 6;
  google.protobuf.Timestamp started = 7;
  google.protobuf.Timestamp finished = 8;
  pfs_v2.Commit output_commit = 9;
  JobState state = 10;
  string reason = 11;  // reason explains why the job is in the current state
  Service service = 12;                        // requires ListJobRequest.Full
  Spout spout = 13;                            // requires ListJobRequest.Full
  pfs_v2.Repo output_repo = 14;
  string output_branch = 15;                   // requires ListJobRequest.Full
  uint64 restart = 16;
  int64 data_processed = 17;
  int64 data_skipped = 18;
  int64 data_failed = 19;
  int64 data_recovered = 20;
  int64 data_total = 21;
  ProcessStats stats = 22;
  repeated WorkerStatus worker_status = 23;
  ResourceSpec resource_requests = 24;         // requires ListJobRequest.Full
  ResourceSpec resource_limits = 25;           // requires ListJobRequest.Full
  ResourceSpec sidecar_resource_limits = 26;   // requires ListJobRequest.Full
  Input input = 27;                            // requires ListJobRequest.Full
  pfs_v2.BranchInfo new_branch = 28;
  string salt = 30;                            // requires ListJobRequest.Full
  DatumSetSpec datum_set_spec = 31;                   // requires ListJobRequest.Full
  google.protobuf.Duration datum_timeout = 32; // requires ListJobRequest.Full
  google.protobuf.Duration job_timeout = 33;   // requires ListJobRequest.Full
  int64 datum_tries = 34;                      // requires ListJobRequest.Full
  SchedulingSpec scheduling_spec = 35;         // requires ListJobRequest.Full
  string pod_spec = 36;                        // requires ListJobRequest.Full
  string pod_patch = 37;                       // requires ListJobRequest.Full
}

enum WorkerState {
  POD_RUNNING = 0;
  POD_SUCCESS = 1;
  POD_FAILED = 2;
}

message Worker {
  string name = 1;
  WorkerState state = 2;
}

message Pipeline {
  string name = 1;
}

enum PipelineState {
  // There is a StoredPipelineInfo + spec commit, but no RC
  // This happens when a pipeline has been created but not yet picked up by a
  // PPS server.
  PIPELINE_STARTING = 0;
  // A pipeline has a spec commit and a service + RC
  // This is the normal state of a pipeline.
  PIPELINE_RUNNING = 1;
  // Equivalent to STARTING (there is a StoredPipelineInfo + commit, but no RC)
  // After some error caused runPipeline to exit, but before the pipeline is
  // re-run. This is when the exponential backoff is in effect.
  PIPELINE_RESTARTING = 2;
  // The pipeline has encountered unrecoverable errors and is no longer being
  // retried. It won't leave this state until the pipeline is updated.
  PIPELINE_FAILURE = 3;
  // The pipeline has been explicitly paused by the user (the pipeline spec's
  // Stopped field should be true if the pipeline is in this state)
  PIPELINE_PAUSED = 4;
  // The pipeline is fully functional, but there are no commits to process.
  PIPELINE_STANDBY = 5;
  // The pipeline's workers are crashing, or failing to come up, this may
  // resolve itself, the pipeline may make progress while in this state if the
  // problem is only being experienced by some workers.
  PIPELINE_CRASHING = 6;
}

// StoredPipelineInfo is proto for each pipeline that Pachd stores in the
// database. It tracks the state of the pipeline, and points to its metadata in
// PFS (and, by pointing to a PFS commit, de facto tracks the pipeline's
// version)
message StoredPipelineInfo {
  Pipeline pipeline = 1;
  uint64 version = 2;
  pfs_v2.Commit spec_commit = 3; // The first spec commit for this version of the pipeline
  bool stopped = 4;
  PipelineState state = 5;
  string reason = 6;
  map<int32, int32> job_counts = 7;
  string auth_token = 8;
  JobState last_job_state = 9;

  // parallelism tracks the literal number of workers that this pipeline should
  // run.
  uint64 parallelism = 10;

  // The pipeline type is stored here so that we can internally know the type of
  // the pipeline without loading the spec from PFS - this field is not in the
  // full PipelineInfo as that would be redundant.
  enum PipelineType {
    PIPELINE_TYPE_TRANSFORM = 0;
    PIPELINE_TYPE_SPOUT = 1;
    PIPELINE_TYPE_SERVICE = 2;
  }
  PipelineType type = 11;
}

message PipelineInfo {
  Pipeline pipeline = 1;
  uint64 version = 2;
  Transform transform = 3;
  // tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
  // when running in a kubernetes cluster on which kubeflow has been installed.
  // Exactly one of 'tf_job' and 'transform' should be set
  TFJob tf_job = 4 [(gogoproto.customname) = "TFJob"];
  ParallelismSpec parallelism_spec = 5;
  Egress egress = 6;
  google.protobuf.Timestamp created_at = 7;

  // state indicates the current state of the pipeline. This is not stored in
  // PFS along with the rest of this data structure--PPS.InspectPipeline fills
  // it in
  PipelineState state = 8;
  // same for stopped field
  bool stopped = 9;
  string recent_error = 10;

  int64 workers_requested = 11;
  int64 workers_available = 12;

  // job_counts and last_job_state indicates the number of jobs within this
  // pipeline in a given state and the state of the most recently created job,
  // respectively. This is not stored in PFS along with the rest of this data
  // structure--PPS.InspectPipeline fills it in from the StoredPipelineInfo.
  map<int32, int32> job_counts = 13;
  JobState last_job_state = 14;

  string output_branch = 15;
  ResourceSpec resource_requests = 16;
  ResourceSpec resource_limits = 17;
  ResourceSpec sidecar_resource_limits = 18;
  Input input = 19;
  string description = 20;
  string cache_size = 21;
  string salt = 23;

  // reason includes any error messages associated with a failed pipeline
  string reason = 24;
  int64 max_queue_size = 25;
  Service service = 26;
  Spout spout = 27;
  DatumSetSpec datum_set_spec = 28;
  google.protobuf.Duration datum_timeout = 29;
  google.protobuf.Duration job_timeout = 30;
  bool standby = 31;
  int64 datum_tries = 32;
  SchedulingSpec scheduling_spec = 33;
  string pod_spec = 34;
  string pod_patch = 35;
  bool s3_out = 36;
  Metadata metadata = 37;
  string reprocess_spec = 38;
  // Set at call time, not stored in etcd
  int64 unclaimed_tasks = 39;
  string worker_rc = 40;
  bool autoscaling = 41;
}

message PipelineInfos {
  repeated PipelineInfo pipeline_info = 1;
}

message Jobset {
  string id = 1 [(gogoproto.customname) = "ID"];
}

message InspectJobsetRequest {
  Jobset jobset = 1;
  bool wait = 2; // When true, wait until all jobs in the set are finished
}

message InspectJobRequest {
  // Callers should set either Job or OutputCommit, not both.
  Job job = 1;
  bool wait = 2; // wait until state is either FAILURE or SUCCESS
  bool full = 3;
}

message ListJobRequest {
  Pipeline pipeline = 1;                // nil means all pipelines
  repeated pfs_v2.Commit input_commit = 2; // nil means all inputs

  // History indicates return jobs from historical versions of pipelines
  // semantics are:
  // 0: Return jobs from the current version of the pipeline or pipelines.
  // 1: Return the above and jobs from the next most recent version
  // 2: etc.
  //-1: Return jobs from all historical versions.
  int64 history = 4;

  // Full indicates whether the result should include all pipeline details in
  // each JobInfo, or limited information including name and status, but
  // excluding information in the pipeline spec. Leaving this "false" can make
  // the call significantly faster in clusters with a large number of pipelines
  // and jobs.
  // Note that if 'input_commit' is set, this field is coerced to "true"
  bool full = 5;

  // A jq program string for additional result filtering
  string jqFilter = 6;
}

// Streams open jobs until canceled
message SubscribeJobRequest {
  Pipeline pipeline = 1;
  bool full = 2; // Same as ListJobRequest.Full
}

message DeleteJobRequest {
  Job job = 1;
}

message StopJobRequest {
  Job job = 1;
  string reason = 3;
}

message UpdateJobStateRequest {
  Job job = 1;
  JobState state = 2;
  string reason = 3;
  uint64 restart = 5;
  int64 data_processed = 6;
  int64 data_skipped = 7;
  int64 data_failed = 8;
  int64 data_recovered = 9;
  int64 data_total = 10;
  ProcessStats stats = 11;
}

message GetLogsRequest {
  // The pipeline from which we want to get logs (required if the job in 'job'
  // was created as part of a pipeline. To get logs from a non-orphan job
  // without the pipeline that created it, you need to use ElasticSearch).
  Pipeline pipeline = 1;

  // The job from which we want to get logs.
  Job job = 2;

  // Names of input files from which we want processing logs. This may contain
  // multiple files, to query pipelines that contain multiple inputs. Each
  // filter may be an absolute path of a file within a pps repo, or it may be
  // a hash for that file (to search for files at specific versions)
  repeated string data_filters = 3;

  Datum datum = 4;

  // If true get logs from the master process
  bool master = 5;

  // Continue to follow new logs as they become available.
  bool follow = 6;

  // If nonzero, the number of lines from the end of the logs to return.  Note:
  // tail applies per container, so you will get tail * <number of pods> total
  // lines back.
  int64 tail = 7;

  // UseLokiBackend causes the logs request to go through the loki backend
  // rather than through kubernetes. This behavior can also be achieved by
  // setting the LOKI_LOGGING feature flag.
  bool use_loki_backend = 8;

  // Since specifies how far in the past to return logs from. It defaults to 24 hours.
  google.protobuf.Duration since = 9;
}

// LogMessage is a log line from a PPS worker, annotated with metadata
// indicating when and why the line was logged.
message LogMessage {
  // The job and pipeline for which a PFS file is being processed (if the job
  // is an orphan job, pipeline name and ID will be unset)
  string pipeline_name = 1;
  string job_id = 2 [(gogoproto.customname) = "JobID"];
  string worker_id = 3 [(gogoproto.customname) = "WorkerID"];
  string datum_id = 4 [(gogoproto.customname) = "DatumID"];
  bool master = 5;

  // The PFS files being processed (one per pipeline/job input)
  repeated InputFile data = 6;

  // User is true if log message comes from the users code.
  bool user = 7;

  // The message logged, and the time at which it was logged
  google.protobuf.Timestamp ts = 8;
  string message = 9;
}

message RestartDatumRequest {
  Job job = 1;
  repeated string data_filters = 2;
}

message InspectDatumRequest {
  Datum datum = 1;
}

message ListDatumRequest {
  // Job and Input are two different ways to specify the datums you want.
  // Only one can be set.
  // Job is the job to list datums from.
  Job job = 1;
  // Input is the input to list datums from.
  // The datums listed are the ones that would be run if a pipeline was created
  // with the provided input.
  Input input = 2;
  // TODO:
  //int64 page_size = 2;
  //int64 page = 3;
}

// DatumSetSpec specifies how a pipeline should split its datums into datum sets.
message DatumSetSpec {
  // number, if nonzero, specifies that each datum set should contain `number`
  // datums. Datum sets may contain fewer if the total number of datums don't
  // divide evenly.
  int64 number = 1;
  // size_bytes, if nonzero, specifies a target size for each datum set.
  // Datum sets may be larger or smaller than size_bytes, but will usually be
  // pretty close to size_bytes in size.
  int64 size_bytes = 2;

  // per_worker, if nonzero, specifies how many datum sets should be created
  // for each worker. It can't be set with number or size_bytes.
  int64 per_worker = 3;
}

message SchedulingSpec {
  map<string, string> node_selector = 1;
  string priority_class_name = 2;
}

message CreatePipelineRequest {
  Pipeline pipeline = 1;
  // tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
  // when running in a kubernetes cluster on which kubeflow has been installed.
  // Exactly one of 'tf_job' and 'transform' should be set
  TFJob tf_job = 2 [(gogoproto.customname) = "TFJob"];
  Transform transform = 3;
  ParallelismSpec parallelism_spec = 4;
  Egress egress = 5;
  bool update = 6;
  string output_branch = 7;
  // s3_out, if set, requires a pipeline's user to write to its output repo
  // via Pachyderm's s3 gateway (if set, workers will serve Pachyderm's s3
  // gateway API at http://<pipeline>-s3.<namespace>/<job id>.out/my/file).
  // In this mode /pfs_v2/out won't be walked or uploaded, and the s3 gateway
  // service in the workers will allow writes to the job's output commit
  bool s3_out = 8;
  ResourceSpec resource_requests = 9;
  ResourceSpec resource_limits = 10;
  ResourceSpec sidecar_resource_limits = 11;
  Input input = 12;
  string description = 13;
  string cache_size = 14;
  // Reprocess forces the pipeline to reprocess all datums.
  // It only has meaning if Update is true
  bool reprocess = 16;
  int64 max_queue_size = 17;
  Service service = 18;
  Spout spout = 19;
  DatumSetSpec datum_set_spec = 20;
  google.protobuf.Duration datum_timeout = 21;
  google.protobuf.Duration job_timeout = 22;
  string salt = 23;
  bool standby = 24;
  int64 datum_tries = 25;
  SchedulingSpec scheduling_spec = 26;
  string pod_spec = 27; // deprecated, use pod_patch below
  string pod_patch = 28; // a json patch will be applied to the pipeline's pod_spec before it's created;
  pfs_v2.Commit spec_commit = 29;
  Metadata metadata = 30;
  string reprocess_spec = 31;
  bool autoscaling = 32;
}

message InspectPipelineRequest {
  Pipeline pipeline = 1;
}

message ListPipelineRequest {
  // If non-nil, only return info about a single pipeline, this is redundant
  // with InspectPipeline unless history is non-zero.
  Pipeline pipeline = 1;
  // History indicates how many historical versions you want returned. Its
  // semantics are:
  // 0: Return the current version of the pipeline or pipelines.
  // 1: Return the above and the next most recent version
  // 2: etc.
  //-1: Return all historical versions.
  int64 history = 2;

  // Return PipelineInfos with incomplete data if the pipeline spec cannot be
  // retrieved. Incomplete PipelineInfos will have a nil Transform field, but
  // will have the fields present in StoredPipelineInfo.
  bool allow_incomplete = 3;

  // A jq program string for additional result filtering
  string jqFilter = 4;
}

message DeletePipelineRequest {
  Pipeline pipeline = 1;
  bool all = 2;
  bool force = 3;
  bool keep_repo = 4;
}

message StartPipelineRequest {
  Pipeline pipeline = 1;
}

message StopPipelineRequest {
  Pipeline pipeline = 1;
}

message RunPipelineRequest {
  Pipeline pipeline = 1;
  repeated pfs_v2.Commit provenance = 2;
  string job_id = 3 [(gogoproto.customname) = "JobID"];
}

message RunCronRequest {
  Pipeline pipeline = 1;
}

message CreateSecretRequest {
  bytes file = 1;
}

message DeleteSecretRequest {
  Secret secret = 1;
}

message InspectSecretRequest {
  Secret secret = 1;
}

message Secret {
  string name = 1;
}

message SecretInfo {
  Secret secret = 1;
  string type = 2;
  google.protobuf.Timestamp creation_timestamp = 3;
}

message SecretInfos {
  repeated SecretInfo secret_info = 1;
}

message ActivateAuthRequest {}
message ActivateAuthResponse {}

service API {
  rpc InspectJob(InspectJobRequest) returns (JobInfo) {}
  rpc InspectJobset(InspectJobsetRequest) returns (stream JobInfo) {}
  // ListJob returns information about current and past Pachyderm jobs.
  rpc ListJob(ListJobRequest) returns (stream JobInfo) {}
  rpc SubscribeJob(SubscribeJobRequest) returns (stream JobInfo) {}
  rpc DeleteJob(DeleteJobRequest) returns (google.protobuf.Empty) {}
  rpc StopJob(StopJobRequest) returns (google.protobuf.Empty) {}
  rpc InspectDatum(InspectDatumRequest) returns (DatumInfo) {}
  // ListDatum returns information about each datum fed to a Pachyderm job
  rpc ListDatum(ListDatumRequest) returns (stream DatumInfo) {}
  rpc RestartDatum(RestartDatumRequest) returns (google.protobuf.Empty) {}

  rpc CreatePipeline(CreatePipelineRequest) returns (google.protobuf.Empty) {}
  rpc InspectPipeline(InspectPipelineRequest) returns (PipelineInfo) {}
  rpc ListPipeline(ListPipelineRequest) returns (PipelineInfos) {}
  rpc DeletePipeline(DeletePipelineRequest) returns (google.protobuf.Empty) {}
  rpc StartPipeline(StartPipelineRequest) returns (google.protobuf.Empty) {}
  rpc StopPipeline(StopPipelineRequest) returns (google.protobuf.Empty) {}
  rpc RunPipeline(RunPipelineRequest) returns (google.protobuf.Empty) {}
  rpc RunCron(RunCronRequest) returns (google.protobuf.Empty) {}

  rpc CreateSecret(CreateSecretRequest) returns (google.protobuf.Empty) {}
  rpc DeleteSecret(DeleteSecretRequest) returns (google.protobuf.Empty) {}
  rpc ListSecret(google.protobuf.Empty) returns (SecretInfos) {}
  rpc InspectSecret(InspectSecretRequest) returns (SecretInfo) {}

  // DeleteAll deletes everything
  rpc DeleteAll(google.protobuf.Empty) returns (google.protobuf.Empty) {}
  rpc GetLogs(GetLogsRequest) returns (stream LogMessage) {}

  // An internal call that causes PPS to put itself into an auth-enabled state
  // (all pipeline have tokens, correct permissions, etcd)
  rpc ActivateAuth(ActivateAuthRequest) returns (ActivateAuthResponse) {}

  // An internal call used to move a job from one state to another
  rpc UpdateJobState(UpdateJobStateRequest) returns(google.protobuf.Empty) {}
}
